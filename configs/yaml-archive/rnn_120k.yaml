## Where the samples will be written
# Not used
save_data: data/inner/onmt/example

# Training files
data:
    corpus_1:
        path_src: data/cleaned/neural/train.src
        path_tgt: data/cleaned/neural/train.tgt
        transforms: [filtertoolong]
    valid:
        path_src: data/cleaned/neural/dev.src
        path_tgt: data/cleaned/neural/dev.tgt
        transforms: [filtertoolong]


# Vocabulary files, generated by onmt_build_vocab
#src_vocab: vocab.src
#tgt_vocab: vocab.tgt

# Vocabulary size - should be the same as in sentence piece
src_vocab_size: 50000
tgt_vocab_size: 50000

# Filter out source/target longer than n if [filtertoolong] enabled
src_seq_length: 150
src_seq_length: 150

# Tokenization options
src_subword_model: source.model
tgt_subword_model: target.model

# Where to save the log file and the output models/checkpoints

#log_file: train.log
#save_model: model

# Stop training if it does not improve after n validations
# early_stopping: 4

# Default: 5000 - Save a model checkpoint for each n
save_checkpoint_steps: 1000

# To save space, limit checkpoints to last n
#keep_checkpoint: 3

seed: 3435

# Default: 100000 - Train the model to max n steps 
# Increase to 200000 or more for large datasets
# For fine-tuning, add up the required steps to the original steps
train_steps: 240000

# Default: 10000 - Run validation after n steps
valid_steps: 2000

# Default: 4000 - for large datasets, try up to 8000
warmup_steps: 1000
report_every: 100

# Number of GPUs, and IDs of GPUs
world_size: 1
gpu_ranks: [0]

###############################################################################
######## RECOMMENDED SETUP FOR TRANSFORMER ARCHITECTURE #######################


# Batching
# bucket_size: 262144
# num_workers: 0  # Default: 2, set to 0 when RAM out of memory
# batch_type: "tokens"
# batch_size: 4096   # Tokens per batch, change when CUDA out of memory
# valid_batch_size: 2048
# max_generator_batches: 2
# accum_count: [4]
# accum_steps: [0]

# Optimization
# model_dtype: "fp16"
# optim: "adam"
# learning_rate: 2
# # warmup_steps: 8000
# decay_method: "noam"
# adam_beta2: 0.998
# max_grad_norm: 0
# label_smoothing: 0.1
# param_init: 0
# param_init_glorot: true
# normalization: "tokens"

# Model

# encoder_type: transformer
# decoder_type: transformer
# position_encoding: true
# enc_layers: 6
# dec_layers: 6
# heads: 8
# hidden_size: 512
# word_vec_size: 512
# transformer_ff: 2048
# dropout_steps: [0]
# dropout: [0.1]
# attention_dropout: [0.1]

###############################################################################
######## RNN SPECIFIC ##################### ###################################

#encoder_type: brnn
# [--encoder_type {rnn,brnn,ggnn,mean,transformer,cnn,transformer_lm}]
# Default: “rnn”

#decoder_type: rnn
# [--decoder_type {rnn,transformer,cnn,transformer_lm}]
# Default: “rnn”

#rnn_type: LSTM
# [--rnn_type {LSTM,GRU,SRU}]
# Default: “LSTM”

# ---------------------------------------------

# --hidden_size, -hidden_size
#     Size of rnn hidden states. Overwrites enc_hid_size and dec_hid_size
#     Default: -1

# --enc_hid_size, -enc_hid_size
#     Size of encoder rnn hidden states.
#     Default: 500

# --dec_hid_size, -dec_hid_size
#     Size of decoder rnn hidden states.
#     Default: 500

hidden_size: 500
#enc_hid_size: 100
#dec_hid_size: 100

# ---------------------------------------------

# --enc_layers, -enc_layers
#     Number of layers in the encoder
#     Default: 2

# --dec_layers, -dec_layers
#     Number of layers in the decoder
#     Default: 2

enc_layers: 4
dec_layers: 4

# ---------------------------------------------

# TODO doplnit defaults podle https://opennmt.net/OpenNMT-py/options/train.html
# a dal ladit

# --optim, -optim
#     Possible choices: sgd, adagrad, adadelta, adam, sparseadam, adafactor, fusedadam
#     Optimization method.
#     Default: “sgd”

#optim: "adadelta"


#learning_rate: 1.0 


# Starting learning rate. Recommended settings: sgd = 1, adagrad = 0.1,
# adadelta = 1, adam = 0.001
#decay_method: "noam"

# --learning_rate_decay, -learning_rate_decay
#     If update_learning_rate, decay learning rate by this much if steps have gone past start_decay_steps
#     Default: 0.5

# --start_decay_steps, -start_decay_steps
#     Start decaying every decay_steps after start_decay_steps
#     Default: 50000

# --decay_steps, -decay_steps
#     Decay every decay_steps
#     Default: 10000

# --decay_method, -decay_method
#     Possible choices: noam, noamwd, rsqrt, none
#     Use a custom decay rate.
#     Default: “none”

# ---------------------------------------------

batch_size: 256
# default: 64

#batch_type: "sents"
#generator_function: "softmax"


# ---------------------------------------------

#replace_unk: true

# ----------------------------------
# THIS NOT, IT WAS NOT SUCCESSFUL

# --copy_attn, -copy_attn
#     Train copy attention layer.
#     Default: False

# --copy_attn_type, -copy_attn_type
#     Possible choices: dot, general, mlp, none
#     The copy attention type to use. Leave as None to use the same as -global_attention.


#copy_attn: true

# --copy_attn_force, -copy_attn_force
#     When available, train to copy.
#     Default: False

# copy_attn_force: When available, train to copy.

# ------------------------------

# global_attention: "general"

# ---------------------------------------------

word_vec_size: 128
# Default: 500


# shared embeddings need also shared vocab
share_embeddings: true
share_vocab: true

# --share_embeddings, -share_embeddings
#     Share the word embeddings between encoder and decoder. Need to use shared dictionary for this option.
#     Default: False


