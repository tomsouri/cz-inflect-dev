accum_count:
- 4
accum_steps:
- 0
adagrad_accumulator_init: 0
adam_beta1: 0.9
adam_beta2: '0.998'
alignment_heads: 0
alignment_layer: -3
apex_opt_level: ''
attention_dropout:
- 0.1
average_decay: 0
average_every: 1
batch_size: '4096'
batch_size_multiple: 1
batch_type: tokens
bidir_edges: true
bridge_extra_node: true
bucket_size: '262144'
bucket_size_increment: 0
bucket_size_init: -1
cnn_kernel_width: 3
config: example50.yaml
data: '{''corpus_1'': {''path_src'': ''data/cleaned/neural/train.src'', ''path_tgt'':
  ''data/cleaned/neural/train.tgt'', ''transforms'': [''filtertoolong'']}, ''valid'':
  {''path_src'': ''data/cleaned/neural/dev_large.src'', ''path_tgt'': ''data/cleaned/neural/dev_large.tgt'',
  ''transforms'': [''filtertoolong'']}}'
data_type: text
dec_hid_size: 500
dec_layers: '6'
decay_method: noam
decay_steps: 10000
decoder_type: transformer
dropout:
- 0.1
dropout_steps:
- 0
early_stopping: '4'
enc_hid_size: 500
enc_layers: '6'
encoder_type: transformer
exp: ''
exp_host: ''
feat_merge: concat
feat_vec_exponent: 0.7
feat_vec_size: -1
generator_function: softmax
global_attention: general
global_attention_function: softmax
gpu_backend: nccl
gpu_ranks:
- 0
gpu_verbose_level: 0
heads: '8'
hidden_size: '512'
input_feed: 1
insert_ratio: 0.0
keep_checkpoint: -1
label_smoothing: '0.1'
lambda_align: 0.0
lambda_coverage: 0.0
layers: -1
learning_rate: '2'
learning_rate_decay: 0.5
lm_prior_lambda: 0.0
lm_prior_tau: 1.0
log_file: train50.log
log_file_level: '0'
loss_scale: 0
mask_length: subword
mask_ratio: 0.0
master_ip: localhost
master_port: 10000
max_generator_batches: '2'
max_grad_norm: '0'
max_relative_positions: 0
model_dtype: fp16
model_task: seq2seq
model_type: text
n_edge_types: 2
n_node: 2
n_sample: 0
n_steps: 2
normalization: tokens
num_workers: '0'
optim: adam
param_init: '0'
param_init_glorot: 'True'
permute_sent_ratio: 0.0
poisson_lambda: 3.0
pos_ffn_activation_fn: relu
position_encoding: 'True'
prefetch_factor: 200
random_ratio: 0.0
replace_length: -1
report_every: '100'
reset_optim: none
reversible_tokenization: joiner
rnn_type: LSTM
rotate_ratio: 0.0
save_checkpoint_steps: '1000'
save_data: data/inner/onmt/example
save_model: data/inner/onmt/run/model50
seed: '3435'
self_attn_type: scaled-dot
skip_empty_level: warning
src_ggnn_size: 0
src_onmttok_kwargs: '{''mode'': ''none''}'
src_prefix: ''
src_seq_length: '150'
src_subword_alpha: 0
src_subword_model: source.model
src_subword_nbest: 1
src_subword_type: none
src_subword_vocab: ''
src_vocab: data/inner/onmt/run/example.vocab.src
src_vocab_size: '50000'
src_vocab_threshold: 0
src_word_vec_size: 500
src_words_min_frequency: 0
start_decay_steps: 50000
state_dim: 512
switchout_temperature: 1.0
tensorboard_log_dir: runs/onmt
tgt_onmttok_kwargs: '{''mode'': ''none''}'
tgt_prefix: ''
tgt_seq_length: 200
tgt_subword_alpha: 0
tgt_subword_model: target.model
tgt_subword_nbest: 1
tgt_subword_type: none
tgt_subword_vocab: ''
tgt_vocab: data/inner/onmt/run/example.vocab.tgt
tgt_vocab_size: '50000'
tgt_vocab_threshold: 0
tgt_word_vec_size: 500
tgt_words_min_frequency: 0
tokendrop_temperature: 1.0
tokenmask_temperature: 1.0
train_eval_steps: 200
train_from: ''
train_metrics: []
train_steps: '50000'
transformer_ff: '2048'
transforms: []
truncated_decoder: 0
valid_batch_size: '2048'
valid_metrics: []
valid_steps: '5000'
vocab_size_multiple: 8
warmup_steps: '4000'
word_vec_size: '512'
world_size: '1'
