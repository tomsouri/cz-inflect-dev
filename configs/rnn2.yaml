## Where the samples will be written
# Not used
save_data: data/inner/onmt/example

# Training files
data:
    corpus_1:
        path_src: data/cleaned/neural/train.src
        path_tgt: data/cleaned/neural/train.tgt
        transforms: [filtertoolong]
    valid:
        path_src: data/cleaned/neural/dev_medium.src
        path_tgt: data/cleaned/neural/dev_medium.tgt
        transforms: [filtertoolong]


# Vocabulary files, generated by onmt_build_vocab
#src_vocab: vocab.src
#tgt_vocab: vocab.tgt

# Vocabulary size - should be the same as in sentence piece
src_vocab_size: 50000
tgt_vocab_size: 50000

# Filter out source/target longer than n if [filtertoolong] enabled
src_seq_length: 150
src_seq_length: 150

# Tokenization options
#src_subword_model: source.model
#tgt_subword_model: target.model

# Where to save the log file and the output models/checkpoints

#log_file: train.log
#save_model: model

# Stop training if it does not improve after n validations
# early_stopping: 4

# Default: 5000 - Save a model checkpoint for each n
save_checkpoint_steps: 20000

# To save space, limit checkpoints to last n
#keep_checkpoint: 3

seed: 3435

# Default: 100000 - Train the model to max n steps 
# Increase to 200000 or more for large datasets
# For fine-tuning, add up the required steps to the original steps

train_steps: 260000

# Default: 10000 - Run validation after n steps
valid_steps: 20000

# Default: 4000 - for large datasets, try up to 8000
warmup_steps: 4000

report_every: 1000

# Number of GPUs, and IDs of GPUs
world_size: 1
gpu_ranks: [0]

###############################################################################
######## RECOMMENDED SETUP FOR TRANSFORMER ARCHITECTURE #######################


# Batching
# bucket_size: 262144
# num_workers: 0  # Default: 2, set to 0 when RAM out of memory
# batch_type: "tokens"
# batch_size: 4096   # Tokens per batch, change when CUDA out of memory
# valid_batch_size: 2048
# max_generator_batches: 2
# accum_count: [4]
# accum_steps: [0]

# Optimization
# model_dtype: "fp16"
# optim: "adam"
# learning_rate: 2
# # warmup_steps: 8000
# decay_method: "noam"
# adam_beta2: 0.998
# max_grad_norm: 0
# label_smoothing: 0.1
# param_init: 0
# param_init_glorot: true
# normalization: "tokens"

# Model

# encoder_type: transformer
# decoder_type: transformer
# position_encoding: true
# enc_layers: 6
# dec_layers: 6
# heads: 8
# hidden_size: 512
# word_vec_size: 512
# transformer_ff: 2048
# dropout_steps: [0]
# dropout: [0.1]
# attention_dropout: [0.1]

###############################################################################
######## RNN SPECIFIC ##################### ###################################
# ChatGPT-generated explanation:
#     RNN (Recurrent Neural Network) Encoder: This is a type of sequential encoder that processes input sequences by passing each token through a series of recurrent nodes, which have a memory that can retain information about previous tokens in the sequence. RNNs are capable of capturing sequential dependencies and can be bidirectional (BRNN) to process the sequence in both forward and backward directions.

#     GGNN (Gated Graph Neural Network) Encoder: This is a type of graph encoder that processes input graphs by propagating information through the graph structure using gated message passing. GGNNs are particularly useful for processing graph-structured data, such as molecular graphs in chemistry.

#     Mean Encoder: This is a simple type of encoder that computes the average of the embeddings of all input tokens to produce a fixed-length representation. This encoder is commonly used in bag-of-words models, where the order of the tokens does not matter.

#     Transformer Encoder: This is a type of encoder that uses self-attention mechanisms to compute a fixed-length representation of the input sequence. The transformer encoder can process the entire sequence in parallel and has been shown to outperform RNNs in many NLP tasks.

#     CNN (Convolutional Neural Network) Encoder: This is a type of encoder that applies convolutional filters to the input sequence to capture local dependencies between neighboring tokens. CNNs are commonly used in image processing, but have also been applied to text processing.

#     Transformer Language Model (LM) Encoder: This is a variant of the transformer encoder that is trained to predict the next token in a sequence given the previous tokens. The transformer LM encoder is used in language modeling tasks, where the goal is to generate coherent text.

# In practice, the choice of encoder type depends on the specific requirements of the task and the characteristics of the input data. For example, RNNs are commonly used in speech recognition tasks, while transformer encoders are preferred in many NLP tasks.

#encoder_type: brnn
# [--encoder_type {rnn,brnn,ggnn,mean,transformer,cnn,transformer_lm}]
# Default: “rnn”

#decoder_type: rnn
# [--decoder_type {rnn,transformer,cnn,transformer_lm}]
# Default: “rnn”

#rnn_type: LSTM
# [--rnn_type {LSTM,GRU,SRU}]
# Default: “LSTM”

# ---------------------------------------------

# --hidden_size, -hidden_size
#     Size of rnn hidden states. Overwrites enc_hid_size and dec_hid_size
#     Default: -1

# --enc_hid_size, -enc_hid_size
#     Size of encoder rnn hidden states.
#     Default: 500

# --dec_hid_size, -dec_hid_size
#     Size of decoder rnn hidden states.
#     Default: 500

#hidden_size: 500
#enc_hid_size: 100
#dec_hid_size: 100

# ---------------------------------------------

# --enc_layers, -enc_layers
#     Number of layers in the encoder
#     Default: 2

# --dec_layers, -dec_layers
#     Number of layers in the decoder
#     Default: 2

#enc_layers: 4
#dec_layers: 4

# ---------------------------------------------

# TODO doplnit defaults podle https://opennmt.net/OpenNMT-py/options/train.html
# a dal ladit

# --optim, -optim
#     Possible choices: sgd, adagrad, adadelta, adam, sparseadam, adafactor, fusedadam
#     Optimization method.
#     Default: “sgd”

#optim: "adadelta"


#learning_rate: 1.0 


# Starting learning rate. Recommended settings: sgd = 1, adagrad = 0.1,
# adadelta = 1, adam = 0.001
#decay_method: "noam"

# --learning_rate_decay, -learning_rate_decay
#     If update_learning_rate, decay learning rate by this much if steps have gone past start_decay_steps
#     Default: 0.5

# --start_decay_steps, -start_decay_steps
#     Start decaying every decay_steps after start_decay_steps
#     Default: 50000

# --decay_steps, -decay_steps
#     Decay every decay_steps
#     Default: 10000

# --decay_method, -decay_method
#     Possible choices: noam, noamwd, rsqrt, none
#     Use a custom decay rate.
#     Default: “none”

# ---------------------------------------------

#batch_size: 20
# default: 64

#batch_type: "sents" # is default
#generator_function: "softmax"


# --------------------------------------------
encoder_type: brnn
decoder_type: rnn
rnn_type: LSTM

hidden_size: 200
enc_layers: 3
dec_layers: 3
batch_size: 256
copy_attn: false #(nevím)

word_vec_size: 128

optim: adam
learning_rate: 0.001

#global_attention: mlp

#share_embeddings: true
#share_vocab: true 

valid_batch_size: 16384 # vic ne, na 65536 uz to spadlo na cuda out of memory

# ---------------------------------------------
# learning_rate
#    Starting learning rate. Recommended settings: sgd = 1, adagrad = 0.1, adadelta = 1, adam = 0.001
# Default: 1.0


#replace_unk: true

# ----------------------------------
# THIS NOT, IT WAS NOT SUCCESSFUL

# --copy_attn, -copy_attn
#     Train copy attention layer.
#     Default: False

# --copy_attn_type, -copy_attn_type
#     Possible choices: dot, general, mlp, none
#     The copy attention type to use. Leave as None to use the same as -global_attention.


#copy_attn: true

# --copy_attn_force, -copy_attn_force
#     When available, train to copy.
#     Default: False

# copy_attn_force: When available, train to copy.

# ------------------------------

#word_vec_size: 96
# Default: 500

# shared embeddings need also shared vocab
share_embeddings: true
share_vocab: true

# --share_embeddings, -share_embeddings
#     Share the word embeddings between encoder and decoder. Need to use shared dictionary for this option.
#     Default: False

################################################################################

# --bridge, -bridge

#     Have an additional layer between the last encoder state and the first decoder state

#     Default: False

################################################################################

# graph-encoder things: do not use, does not seem to be useful for our task

################################################################################

# Model- Attention
# --global_attention, -global_attention
#     Possible choices: dot, general, mlp, none
#     The attention type to use: dotprod or general (Luong) or MLP (Bahdanau)
#     Default: “general”

# --global_attention_function, -global_attention_function
#     Possible choices: softmax, sparsemax
#     Default: “softmax”

# --self_attn_type, -self_attn_type
#     Self attention type in Transformer decoder layer – currently “scaled-dot” or “average”
#     Default: “scaled-dot”

# --max_relative_positions, -max_relative_positions
#     Maximum distance between inputs in relative positions representations. For more detailed information, see: https://arxiv.org/pdf/1803.02155.pdf
#     Default: 0
# ###############################################
# Transformer stuff
#
# --heads, -heads
#     Number of heads for transformer self-attention
#     Default: 8

# --transformer_ff, -transformer_ff
#     Size of hidden transformer feed-forward
#     Default: 2048

# --aan_useffn, -aan_useffn
#     Turn on the FFN layer in the AAN decoder
#     Default: False

# --add_qkvbias, -add_qkvbias
#     Add bias to nn.linear of Query/Key/Value in MHANote: this will add bias
#     to output proj layer too
#     Default: False

################################################################################
