[2022-12-15 09:42:53,915 INFO] Missing transforms field for corpus_1 data, set to default: [].
[2022-12-15 09:42:53,945 WARNING] Corpus corpus_1's weight should be given. We default it to 1 for you.
[2022-12-15 09:42:53,945 INFO] Missing transforms field for valid data, set to default: [].
[2022-12-15 09:42:53,953 INFO] Parsed 2 corpora from -data.
[2022-12-15 09:42:53,984 INFO] Get special vocabs from Transforms: {'src': [], 'tgt': []}.
[2022-12-15 09:42:54,009 INFO] Building model...
[2022-12-15 09:43:32,542 INFO] NMTModel(
  (encoder): RNNEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(80, 500, padding_idx=1)
        )
      )
    )
    (rnn): LSTM(500, 500, num_layers=2, batch_first=True, dropout=0.3)
  )
  (decoder): InputFeedRNNDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(72, 500, padding_idx=1)
        )
      )
    )
    (dropout): Dropout(p=0.3, inplace=False)
    (rnn): StackedLSTM(
      (dropout): Dropout(p=0.3, inplace=False)
      (layers): ModuleList(
        (0): LSTMCell(1000, 500)
        (1): LSTMCell(500, 500)
      )
    )
    (attn): GlobalAttention(
      (linear_in): Linear(in_features=500, out_features=500, bias=False)
      (linear_out): Linear(in_features=1000, out_features=500, bias=False)
    )
  )
  (generator): Sequential(
    (0): Linear(in_features=500, out_features=72, bias=True)
    (1): Cast()
    (2): LogSoftmax(dim=-1)
  )
)
[2022-12-15 09:43:32,542 INFO] encoder: 4048000
[2022-12-15 09:43:32,542 INFO] decoder: 5830072
[2022-12-15 09:43:32,542 INFO] * number of parameters: 9878072
[2022-12-15 09:43:32,542 INFO]  * src vocab size = 80
[2022-12-15 09:43:32,542 INFO]  * tgt vocab size = 72
/auto/brno2/home/souradat/rp-sourada/czech-automatic-inflection/.venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[2022-12-15 09:43:32,585 INFO] Starting training on GPU: [0]
[2022-12-15 09:43:32,585 INFO] Start training loop and validate every 500 steps...
[2022-12-15 09:44:00,152 INFO] Step 50/ 1000; acc: 7.4; ppl: 361.5; xent: 5.9; lr: 1.00000; sents:    3200; bsz:  965/ 876/64; 1751/1588 tok/s;     28 sec;
[2022-12-15 09:44:01,731 INFO] Step 100/ 1000; acc: 10.9; ppl:  72.8; xent: 4.3; lr: 1.00000; sents:    3200; bsz:  963/ 886/64; 30505/28062 tok/s;     29 sec;
[2022-12-15 09:44:03,020 INFO] Step 150/ 1000; acc: 15.9; ppl:  35.1; xent: 3.6; lr: 1.00000; sents:    3200; bsz:  778/ 702/64; 30187/27248 tok/s;     30 sec;
[2022-12-15 09:44:04,613 INFO] Step 200/ 1000; acc: 24.6; ppl:  19.5; xent: 3.0; lr: 1.00000; sents:    3200; bsz:  951/ 887/64; 29868/27856 tok/s;     32 sec;
[2022-12-15 09:44:06,123 INFO] Step 250/ 1000; acc: 30.5; ppl:  14.1; xent: 2.6; lr: 1.00000; sents:    3200; bsz:  934/ 830/64; 30913/27471 tok/s;     34 sec;
[2022-12-15 09:44:07,582 INFO] Step 300/ 1000; acc: 34.6; ppl:  11.8; xent: 2.5; lr: 1.00000; sents:    3200; bsz:  886/ 805/64; 30369/27591 tok/s;     35 sec;
[2022-12-15 09:44:09,020 INFO] Step 350/ 1000; acc: 34.7; ppl:  11.4; xent: 2.4; lr: 1.00000; sents:    3200; bsz:  867/ 792/64; 30130/27548 tok/s;     36 sec;
[2022-12-15 09:44:10,428 INFO] Step 400/ 1000; acc: 36.0; ppl:  10.7; xent: 2.4; lr: 1.00000; sents:    3200; bsz:  877/ 783/64; 31155/27835 tok/s;     38 sec;
[2022-12-15 09:44:11,875 INFO] Step 450/ 1000; acc: 40.9; ppl:   8.4; xent: 2.1; lr: 1.00000; sents:    3200; bsz:  878/ 812/64; 30337/28060 tok/s;     39 sec;
[2022-12-15 09:44:13,252 INFO] Step 500/ 1000; acc: 44.9; ppl:   7.3; xent: 2.0; lr: 1.00000; sents:    3200; bsz:  832/ 768/64; 30216/27892 tok/s;     41 sec;
[2022-12-15 09:49:20,638 INFO] valid stats calculation and batchs detokenization
                           took: 307.38601446151733 s.
[2022-12-15 09:49:20,639 INFO] Train perplexity: 22.3995
[2022-12-15 09:49:20,639 INFO] Train accuracy: 27.7201
[2022-12-15 09:49:20,639 INFO] Sentences processed: 32000
[2022-12-15 09:49:20,639 INFO] Average bsz:  893/ 814/64
[2022-12-15 09:49:20,639 INFO] Validation perplexity: 6.56849
[2022-12-15 09:49:20,639 INFO] Validation accuracy: 47.3697
[2022-12-15 09:49:20,640 INFO] Saving checkpoint /storage/brno2/home/souradat/rp-sourada/czech-automatic-inflection/data/inner/onmt/run/model_step_500.pt
[2022-12-15 09:49:23,675 INFO] Step 550/ 1000; acc: 53.5; ppl:   5.6; xent: 1.7; lr: 1.00000; sents:    3200; bsz:  900/ 829/64; 145/133 tok/s;    351 sec;
[2022-12-15 09:49:25,122 INFO] Step 600/ 1000; acc: 61.5; ppl:   4.0; xent: 1.4; lr: 1.00000; sents:    3200; bsz:  871/ 801/64; 30097/27678 tok/s;    353 sec;
[2022-12-15 09:49:26,518 INFO] Step 650/ 1000; acc: 73.8; ppl:   2.8; xent: 1.0; lr: 1.00000; sents:    3200; bsz:  850/ 771/64; 30447/27604 tok/s;    354 sec;
[2022-12-15 09:49:27,829 INFO] Step 700/ 1000; acc: 80.8; ppl:   2.1; xent: 0.7; lr: 1.00000; sents:    3200; bsz:  777/ 721/64; 29646/27497 tok/s;    355 sec;
[2022-12-15 09:49:29,213 INFO] Step 750/ 1000; acc: 83.0; ppl:   2.0; xent: 0.7; lr: 1.00000; sents:    3200; bsz:  832/ 768/64; 30059/27735 tok/s;    357 sec;
[2022-12-15 09:49:30,627 INFO] Step 800/ 1000; acc: 84.1; ppl:   1.9; xent: 0.7; lr: 1.00000; sents:    3200; bsz:  851/ 785/64; 30110/27771 tok/s;    358 sec;
[2022-12-15 09:49:32,131 INFO] Step 850/ 1000; acc: 87.4; ppl:   1.6; xent: 0.5; lr: 1.00000; sents:    3200; bsz:  904/ 838/64; 30037/27865 tok/s;    360 sec;
[2022-12-15 09:49:33,428 INFO] Step 900/ 1000; acc: 86.8; ppl:   1.8; xent: 0.6; lr: 1.00000; sents:    3200; bsz:  787/ 695/64; 30345/26779 tok/s;    361 sec;
[2022-12-15 09:49:34,891 INFO] Step 950/ 1000; acc: 91.9; ppl:   1.4; xent: 0.3; lr: 1.00000; sents:    3200; bsz:  899/ 810/64; 30722/27696 tok/s;    362 sec;
[2022-12-15 09:49:36,245 INFO] Step 1000/ 1000; acc: 89.9; ppl:   1.5; xent: 0.4; lr: 1.00000; sents:    3200; bsz:  813/ 740/64; 30021/27332 tok/s;    364 sec;
[2022-12-15 09:54:42,842 INFO] valid stats calculation and batchs detokenization
                           took: 306.5962951183319 s.
[2022-12-15 09:54:42,842 INFO] Train perplexity: 7.26909
[2022-12-15 09:54:42,842 INFO] Train accuracy: 52.7494
[2022-12-15 09:54:42,842 INFO] Sentences processed: 64000
[2022-12-15 09:54:42,842 INFO] Average bsz:  871/ 795/64
[2022-12-15 09:54:42,842 INFO] Validation perplexity: 1.33792
[2022-12-15 09:54:42,842 INFO] Validation accuracy: 91.6969
[2022-12-15 09:54:42,843 INFO] Saving checkpoint /storage/brno2/home/souradat/rp-sourada/czech-automatic-inflection/data/inner/onmt/run/model_step_1000.pt
usage: onmt_translate [-h] [-config CONFIG] [-save_config SAVE_CONFIG] --model
                      MODEL [MODEL ...] [--fp32] [--int8] [--avg_raw_probs]
                      [--data_type DATA_TYPE] --src SRC [-src_feats SRC_FEATS]
                      [--tgt TGT] [--tgt_file_prefix] [--output OUTPUT]
                      [--report_align] [--report_time] [--beam_size BEAM_SIZE]
                      [--ratio RATIO]
                      [--random_sampling_topk RANDOM_SAMPLING_TOPK]
                      [--random_sampling_topp RANDOM_SAMPLING_TOPP]
                      [--random_sampling_temp RANDOM_SAMPLING_TEMP]
                      [--seed SEED] [--length_penalty {none,wu,avg}]
                      [--alpha ALPHA] [--coverage_penalty {none,wu,summary}]
                      [--beta BETA] [--stepwise_penalty]
                      [--min_length MIN_LENGTH] [--max_length MAX_LENGTH]
                      [--block_ngram_repeat BLOCK_NGRAM_REPEAT]
                      [--ignore_when_blocking IGNORE_WHEN_BLOCKING [IGNORE_WHEN_BLOCKING ...]]
                      [--replace_unk] [--ban_unk_token]
                      [--phrase_table PHRASE_TABLE] [--log_file LOG_FILE]
                      [--log_file_level {CRITICAL,ERROR,WARNING,INFO,DEBUG,NOTSET,50,40,30,20,10,0}]
                      [--verbose] [--attn_debug] [--align_debug]
                      [--dump_beam DUMP_BEAM] [--n_best N_BEST] [--with_score]
                      [--batch_size BATCH_SIZE] [--batch_type {sents,tokens}]
                      [--gpu GPU]
                      [-transforms {bart,sentencepiece,bpe,onmt_tokenize,filterfeats,inferfeats,filtertoolong,prefix,switchout,tokendrop,tokenmask} [{bart,sentencepiece,bpe,onmt_tokenize,filterfeats,inferfeats,filtertoolong,prefix,switchout,tokendrop,tokenmask} ...]]
                      [--permute_sent_ratio PERMUTE_SENT_RATIO]
                      [--rotate_ratio ROTATE_RATIO]
                      [--insert_ratio INSERT_RATIO]
                      [--random_ratio RANDOM_RATIO] [--mask_ratio MASK_RATIO]
                      [--mask_length {subword,word,span-poisson}]
                      [--poisson_lambda POISSON_LAMBDA]
                      [--replace_length {-1,0,1}]
                      [-src_subword_model SRC_SUBWORD_MODEL]
                      [-tgt_subword_model TGT_SUBWORD_MODEL]
                      [-src_subword_nbest SRC_SUBWORD_NBEST]
                      [-tgt_subword_nbest TGT_SUBWORD_NBEST]
                      [-src_subword_alpha SRC_SUBWORD_ALPHA]
                      [-tgt_subword_alpha TGT_SUBWORD_ALPHA]
                      [-src_subword_vocab SRC_SUBWORD_VOCAB]
                      [-tgt_subword_vocab TGT_SUBWORD_VOCAB]
                      [-src_vocab_threshold SRC_VOCAB_THRESHOLD]
                      [-tgt_vocab_threshold TGT_VOCAB_THRESHOLD]
                      [-src_subword_type {none,sentencepiece,bpe}]
                      [-tgt_subword_type {none,sentencepiece,bpe}]
                      [-src_onmttok_kwargs SRC_ONMTTOK_KWARGS]
                      [-tgt_onmttok_kwargs TGT_ONMTTOK_KWARGS]
                      [--reversible_tokenization {joiner,spacer}]
                      [--prior_tokenization] [--src_seq_length SRC_SEQ_LENGTH]
                      [--tgt_seq_length TGT_SEQ_LENGTH]
                      [--src_prefix SRC_PREFIX] [--tgt_prefix TGT_PREFIX]
                      [-switchout_temperature SWITCHOUT_TEMPERATURE]
                      [-tokendrop_temperature TOKENDROP_TEMPERATURE]
                      [-tokenmask_temperature TOKENMASK_TEMPERATURE]
onmt_translate: error: unrecognized arguments: Dec 15 09:54:45 CET 2022.txt
